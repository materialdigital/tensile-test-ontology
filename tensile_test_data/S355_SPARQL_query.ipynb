{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S355 Tensile Test SPARQL Queries\n",
    "\n",
    "This Jupyter Notebook provides some examples of SPARQL queries that can be performed to obtain information relevant to tensile testing. \n",
    "An [example dataset of tensile tests performed on an S355 steel](https://github.com/materialdigital/tensile-test-ontology/blob/main/tensile_test_data/S355_data_tto.rdf) is used as a basis. In this Jupyter Notebook, a local triple store is created using the OWLready2 Python package. Within this triple store, the respective ontology and the data are loaded and can be queried afterwards.\n",
    "Accordingly, necessary and useful libraries are imported and helper functions are implemented.\n",
    "The SPARQL queries are read in from especially created files that contain only the SPARQL query body (text of SPARQL query) and can be found in a dedicated [sparql folder](https://github.com/materialdigital/tensile-test-ontology/tree/main/tensile_test_data/sparql).\n",
    "\n",
    "The queries follow the general pattern of SPARQL queries:\n",
    "\n",
    "```SPARQL\n",
    "PREFIX ex: <https://example.org/my/namespace/>\n",
    "\n",
    "SELECT ?s ?p ?o\n",
    "WHERE {\n",
    "    ?s ?p ?o\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import of relevant packages | Definition of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Import relevant and useful packages\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import owlready2 as or2\n",
    "from owlready2 import World\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Definition of helper functions\n",
    "# Function to transform inputs to IRIs.\n",
    "def to_iri(input):\n",
    "    try:\n",
    "        return input.iri\n",
    "    except:\n",
    "        pass\n",
    "    return input\n",
    "\n",
    "# Function to write the result of a SPARQL query into a (pandas) data frame.\n",
    "def sparql_result_to_df(res):\n",
    "    l = []\n",
    "    for row in res:\n",
    "        r = [ to_iri(item)  for item in row]\n",
    "        l.append(r)\n",
    "    return pd.DataFrame(l)\n",
    "\n",
    "\n",
    "def load_ontologies_to_world(*ontology_urls):\n",
    "    \"\"\"\n",
    "    Loads ontologies from the given URLs into an OWLready2 World instance.\n",
    "    \n",
    "    Parameters:\n",
    "        ontology_urls: A variable number of URLs pointing to ontologies.\n",
    "    \n",
    "    Returns:\n",
    "        An OWLready2 World instance containing the loaded ontologies.\n",
    "    \"\"\"\n",
    "    # Create a new World instance for loading ontologies\n",
    "    world = World()\n",
    "    \n",
    "    # Iterate over each provided ontology URL\n",
    "    for url in ontology_urls:\n",
    "        try:\n",
    "            # Fetch the ontology content, following redirects\n",
    "            response = requests.get(url, allow_redirects=True)\n",
    "            response.raise_for_status()  # Check for HTTP errors\n",
    "\n",
    "            # Load the ontology from the response content\n",
    "            world.get_ontology(url).load(fileobj=BytesIO(response.content))\n",
    "        \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Failed to load ontology from {url}: {e}\")\n",
    "    \n",
    "    return world\n",
    "\n",
    "\n",
    "import requests\n",
    "\n",
    "def load_sparql(query_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Loads a SPARQL query file directly from GitHub (raw URL).\n",
    "    \n",
    "    Parameters\n",
    "    ---------\n",
    "    query_name : str\n",
    "        Name of the SPARQL file without extension (.sparql).\n",
    "    \n",
    "    Return\n",
    "    --------\n",
    "    str\n",
    "        Content of the SPARQL file as a string.\n",
    "    \"\"\"\n",
    "    base_url = \"https://raw.githubusercontent.com/materialdigital/tensile-test-ontology/main/tensile_test_data/sparql\"\n",
    "    url = f\"{base_url}/{query_name}.sparql\"\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Datei konnte nicht geladen werden: {url} (Status {response.status_code})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of Sources\n",
    "\n",
    "In the following cell, the sources of ontologies to be parsed as well as the source of the A-Box (example dataset of tensile tests performed on an S355 steel) are specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of links to ontologies, files, etc. to be loaded in the local triple store\n",
    "link_ontology_1 = \"https://w3id.org/pmd/co/\" # PMD Core Ontology (PMDco) as basis for tensile test ontology\n",
    "link_ontology_2 = \"https://w3id.org/pmd/tto/\" # Tensile Test Ontology (TTO)\n",
    "link_data = \"https://raw.githubusercontent.com/materialdigital/tensile-test-ontology/refs/heads/main/tensile_test_data/S355_data_tto.rdf\" # Example data on S355 steel\n",
    "\n",
    "# Loading ontologies and data files (A-Box) in the local triple store\n",
    "triple_store = load_ontologies_to_world(link_ontology_1, link_ontology_2)\n",
    "triple_store.get_ontology(link_data).load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPARQL Query\n",
    "\n",
    "In the following cell, the source, meaning the name, of the SPARQL query file **is to be selected / specified by users**. \n",
    "\n",
    "The query contained in this file will be used for querying in the subsequent cell.\n",
    "\n",
    "### Depiction of Results \n",
    "\n",
    "For a depiction / visualization of results in table format, the module tabulate is used in the following.\n",
    "Furthermore, as the SPARQL query is defined by a dedicated SPARQL query file (link_SPARQL_query), the headers of the result table can be read from the select clause in the query. This way, the result can be double-checked manually and consistency is ensured (did the SPARQL query select statement really address the information I wanted to obtain?). Hence, the following code includes a read in of the information queried for (the terms / concepts / entities addressed using the select clause)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specification of the SPARQL query of interest\n",
    "# Which SPARQL query is to be performed?\n",
    "# Please insert the name of the query (to be found in the \"sparql\" folder)\n",
    "\n",
    "query_name = 'count_all_entities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file from the resource and read the SPARQL query\n",
    "query = load_sparql(query_name)\n",
    "\n",
    "# Execute the SPARQL query\n",
    "res = triple_store.sparql(query)\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "data = sparql_result_to_df(res)\n",
    "\n",
    "# Visualization Part\n",
    "# Step: Extract the terms from the SELECT clause\n",
    "# This regular expression looks for the SELECT or SELECT DISTINCT clause and captures the terms.\n",
    "select_clause_match = re.search(r'SELECT\\s+(DISTINCT\\s+)?(.*?)\\s+WHERE', query, re.DOTALL)\n",
    "\n",
    "if select_clause_match:\n",
    "    select_clause = select_clause_match.group(2)  # Use group(2) to capture the variables\n",
    "    # Split the terms by whitespace and strip any leading or trailing spaces\n",
    "    headers = [term.strip().lstrip('?') for term in select_clause.split() if term.strip().startswith('?')]\n",
    "else:\n",
    "    print(\"No headers were found. Please check the select clause within the SPARQL query.\")\n",
    "\n",
    "# Step: Use the headers in the tabulate print statement\n",
    "# Print the data with tabulate\n",
    "print(tabulate(data, headers=headers, tablefmt='psql', showindex=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform all SPARQL Queries\n",
    "\n",
    "Using the following cell, all SPARQL queries available in the [example sparql folder]() will be performed one after the other automatically. All results are depicted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GitHub API URL to list all files in the SPARQL folder\n",
    "repo_api_url = \"https://api.github.com/repos/materialdigital/tensile-test-ontology/contents/tensile_test_data/sparql\"\n",
    "\n",
    "# Get the JSON response\n",
    "response = requests.get(repo_api_url)\n",
    "files_json = response.json()\n",
    "\n",
    "# Filter only .sparql files\n",
    "query_files = [f['name'] for f in files_json if f['name'].endswith('.sparql')]\n",
    "\n",
    "# Dictionary to store results\n",
    "all_results = {}\n",
    "\n",
    "for query_file in query_files:\n",
    "    query_name = query_file.replace(\".sparql\", \"\")\n",
    "    try:\n",
    "        # Load SPARQL content from GitHub using your existing function\n",
    "        query = load_sparql(query_name)\n",
    "        \n",
    "        # Execute SPARQL query on the triple store\n",
    "        res = triple_store.sparql(query)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = sparql_result_to_df(res)\n",
    "        \n",
    "        # Store in dictionary\n",
    "        all_results[query_name] = df\n",
    "        \n",
    "        # Extract headers from SELECT clause\n",
    "        select_clause_match = re.search(r'SELECT\\s+(DISTINCT\\s+)?(.*?)\\s+WHERE', query, re.DOTALL)\n",
    "        if select_clause_match:\n",
    "            select_clause = select_clause_match.group(2)\n",
    "            headers = [term.strip().lstrip('?') for term in select_clause.split() if term.strip().startswith('?')]\n",
    "        else:\n",
    "            headers = None\n",
    "        \n",
    "        # Print results nicely\n",
    "        print(f\"\\n=== Results for Query: {query_name} ===\")\n",
    "        print(tabulate(df, headers=headers, tablefmt='psql', showindex=True))\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query '{query_name}': {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
